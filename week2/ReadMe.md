â­ WEEK 2 â€” Azure Data Factory Ingestion Pipeline

Theme: Build an orchestrated ingestion pipeline that loads data â†’ ADLS raw/ â†’ (optional) Dataflow transformation â†’ clean/.
Outcome: You finish Week 2 with a fully functional ADF ingestion pipeline, exported JSON, screenshots, and a runbook committed to GitHub.

ğŸ“Œ WEEK 2 LEARNING SOURCES (reference upfront)
ğŸ“ Microsoft Learn (Primary)

These are the exact modules for Week 2:

1ï¸âƒ£ Introduction to Azure Data Factory
https://learn.microsoft.com/learn/modules/intro-to-azure-data-factory/

2ï¸âƒ£ Build your first data factory and pipeline
https://learn.microsoft.com/learn/modules/build-your-first-data-factory/

3ï¸âƒ£ Copy data with pipelines using Data Factory
https://learn.microsoft.com/learn/modules/copy-data-tool-azure-data-factory/

4ï¸âƒ£ Use parameterization in ADF pipelines
https://learn.microsoft.com/learn/modules/parameterize-azure-data-factory/

ğŸ¥ YouTube (Hands-on Demos)

Adam Marczak â€” Azure Data Factory Tutorial for Beginners
https://youtu.be/-QFJjzFZUlU

Adam Marczak â€” Mapping Data Flows (optional)
https://youtu.be/Kv0Z8upGALQ

These are short, practical, and give you instant clarity.

ğŸ’¼ WEEK 2 Deliverables (for reference)

By the end of Week 2, you must produce:

âœ” 1. ADF Ingestion Pipeline (JSON exported)

Linked Services

Datasets

Copy Activity

Parameters: file_name, folder_name

Trigger: Manual + Scheduled

âœ” 2. Documentation

/project/week2-adf/README.md including:

Pipeline diagram (draw.io PNG)

Screenshot of successful pipeline run

Explanation of parameterisation

Runbook: How to rerun pipeline with a new file

âœ” 3. Code + Assets

/src/ingestion/adf/pipeline.json

/src/ingestion/adf/datasets/*.json

/src/ingestion/adf/linkedservices/*.json

âœ” 4. PR into main

Branch: week2-adf-pipeline

ğŸ—“ï¸ WEEK 2 â€” DAY-BY-DAY HANDS-ON GUIDE
ğŸŸ¦ DAY 1 â€” Understand ADF Concepts + Connect ADF to Your Data Lake

Goal: Learn ADF architecture + connect to your ADLS Gen2 from Week 1.

ğŸ“˜ Study (1.5 hrs)

Complete Module 1:

â€œIntroduction to Azure Data Factoryâ€
https://learn.microsoft.com/learn/modules/intro-to-azure-data-factory/

Focus on:

Pipelines

Activities

Linked Services

Datasets

Integration Runtime

Watch (optional but recommended):
ğŸ¥ Adam Marczak ADF overview â†’ https://youtu.be/-QFJjzFZUlU

ğŸ”§ Hands-On (2â€“3 hrs)

Open Azure Portal â†’ Create Data Factory

Name: adf-customer360-yourname

Version: V2

Region: same as your storage account

In ADF Studio â†’ Create Linked Service:

Azure Data Lake Storage Gen2

Authentication: Managed Identity (prefer) or Account Key (for learning only)

Test connection

Verify ADF can browse your storage containers raw/, clean/, curated/.

ğŸ“ Files to Create in Repo
project/week2-adf/linked-services.md
project/week2-adf/screenshots/


Add screenshot:

Linked Service test successful

ğŸŸ¦ DAY 2 â€” Build the First Ingestion Pipeline (Static Copy Activity)

Goal: Create a pipeline that copies a known file (customers.csv) â†’ ADLS raw/.

ğŸ“˜ Study (1â€“1.5 hrs)

Complete Learn Module:

â€œBuild your first data factory and pipelineâ€
https://learn.microsoft.com/learn/modules/build-your-first-data-factory/

ğŸ”§ Hands-On (2.5â€“3 hrs)

In ADF, create pipeline: pl_ingest_customers

Add Copy Data activity

Create Dataset (Source):

Type: Binary or DelimitedText

Point to your local file or sample dataset

Create Dataset (Sink):

ADLS Gen2

Folder: /raw/customers/

Publish and run the pipeline

Confirm in ADLS that the file appears

ğŸ“ Repo Work

Add:

/project/week2-adf/screenshots/run1-success.png
/project/week2-adf/pl_ingest_customers_notes.md

ğŸŸ¦ DAY 3 â€” Add Parameterisation (Make Pipeline Reusable)

Goal: Allow pipeline to ingest any file name & folder dynamically.

ğŸ“˜ Study (1â€“2 hrs)

Complete Learn Module:

â€œUse parameterization in Azure Data Factory pipelinesâ€
https://learn.microsoft.com/learn/modules/parameterize-azure-data-factory/

Notes to focus on:

Pipeline parameters

Dataset parameters

Dynamic Content

@pipeline().parameters.* reference syntax

ğŸ”§ Hands-On (2â€“3 hrs)

Modify your pipeline:

Add parameters:

file_name

source_folder

sink_folder

Modify datasets:

Dataset location = dynamic:

@concat(pipeline().parameters.source_folder, '/', pipeline().parameters.file_name)

Test runs:

Run with parameters:

file_name = customers.csv  
source_folder = staging  
sink_folder = raw/customers

ğŸ“ Repo Work

Add:

/src/ingestion/adf/pipeline-parameterized.json
/project/week2-adf/parameterisation-notes.md

ğŸŸ¦ DAY 4 â€” Add Trigger + Export Pipeline JSON

Goal: Operationalize pipeline + export code for repo versioning.

ğŸ“˜ Study (1 hr)

Learn section:

Triggers in Azure Data Factory (within ADF documentation)
https://learn.microsoft.com/learn/modules/trigger-pipelines-data-factory/

ğŸ”§ Hands-On (3 hrs)

Add a Manual Trigger

Add a Scheduled Trigger (every 24 hours or dummy schedule)

Test both triggers

Export pipeline JSON:

ADF Studio â†’ Manage â†’ ARM Template â†’ Export ARM Template
OR

Use built-in JSON export for pipeline only

Place JSON under:

src/ingestion/adf/pipeline.json
src/ingestion/adf/datasets/
src/ingestion/adf/linkedservices/

ğŸ“ Repo Work

Add images:

/project/week2-adf/screenshots/trigger-setup.png
/project/week2-adf/runbook.md

ğŸŸ¦ DAY 5 â€” Create Pipeline Diagram + Documentation + PR

Goal: Produce engineering-grade documentation and submit Week 2 deliverables.

ğŸ“˜ Study (1 hr)

Revisit Module:

Copy data with Data Factory
https://learn.microsoft.com/learn/modules/copy-data-tool-azure-data-factory/

This gives you insight on performance, throughput, and mapping use cases.

ğŸ”§ Hands-On (3â€“4 hrs)
1. Create a pipeline diagram (use Draw.io)

Include:

Source

Dataset

Copy activity

Sink

Trigger

Parameter flow

Export PNG â†’ save under:

project/week2-adf/diagram.png

2. Write Runbook

Add details:

How to change parameters

How to re-run failed loads

How to add new source folders

Place it at:

project/week2-adf/runbook.md

3. Open PR

Branch: week2-adf-pipeline
Target: main

Include the PR template from Week 1.

â­ WEEK 2 â€” Summary of Expected Deliverables

In your PR, you will submit:

ğŸ“ Code

/src/ingestion/adf/pipeline.json

/src/ingestion/adf/datasets/*.json

/src/ingestion/adf/linkedservices/*.json

ğŸ“˜ Documentation

/project/week2-adf/diagram.png

/project/week2-adf/runbook.md

/project/week2-adf/parameterisation-notes.md

Screenshots folder

âš™ï¸ Pipeline Functionality

Ingests file â†’ ADLS

Uses dynamic file/folder parameters

Has manual + scheduled triggers

Exports JSON via ARM Template

This directly builds into Week 3 (Synapse SQL external tables â†’ analytics layer).
