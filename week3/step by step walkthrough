ğŸ—“ï¸ WEEK 3 â€” DAY-BY-DAY HANDS-ON PLAN (5 study days)
ğŸŸ¦ DAY 1 â€” Synapse Workspace + First Lake Queries

Goal: Create Synapse workspace and query raw data directly in ADLS.

ğŸ“˜ Study (1â€“1.5 hrs)

Complete Microsoft Learn module:

Analyze data with Azure Synapse serverless SQL pools
https://learn.microsoft.com/training/modules/analyze-data-with-sql-serverless/

Focus on:

What serverless SQL is

OPENROWSET

Pay-per-query model

When to use serverless vs dedicated

ğŸ”§ Hands-On (2.5â€“3 hrs)
Step 1 â€” Create Synapse Workspace

Azure Portal â†’ Create resource â†’ Azure Synapse Analytics

Workspace name: syn-customer360-yourname

Link it to your existing ADLS Gen2 account

Set default container (e.g. clean or curated)

Create workspace

Step 2 â€” Query CSV in raw zone (no tables yet)

Open Synapse Studio â†’ Data â†’ SQL scripts â†’ New script

Run this (adjust storage account name):

SELECT TOP 10 *
FROM OPENROWSET(
    BULK 'https://datalakeyourname001.dfs.core.windows.net/raw/customers/customers.csv',
    FORMAT = 'CSV',
    PARSER_VERSION = '2.0',
    FIRSTROW = 2
) AS rows;


âœ… This proves:

Synapse can read your Data Lake

Security & permissions are correct

ğŸ“ Day 1 Deliverables

Commit to GitHub:

src/analytics/synapse/day1_openrowset.sql
project/week3-synapse/screenshots/synapse-query-success.png
project/week3-synapse/day1-notes.md


day1-notes.md should include:

What serverless SQL is

Cost model

Why OPENROWSET is powerful

ğŸŸ¦ DAY 2 â€” Query Parquet + Partitioned Data

Goal: Query Parquet files efficiently and understand partitioning.

ğŸ“˜ Study (1 hr)

Microsoft Learn:

Query data in Azure Data Lake using serverless SQL pools
https://learn.microsoft.com/training/modules/query-data-lake-using-serverless-sql-pools/

Focus on:

Parquet vs CSV

Partition elimination

File pruning

ğŸ”§ Hands-On (3 hrs)
Step 1 â€” Query Parquet (if you have it)

If Week 2 ingested Parquet, run:

SELECT TOP 10 *
FROM OPENROWSET(
    BULK 'https://datalakeyourname001.dfs.core.windows.net/clean/customers/*.parquet',
    FORMAT = 'PARQUET'
) AS customers;

Step 2 â€” Partition-aware query (example)

If your folders are date-partitioned:

SELECT COUNT(*) AS cnt
FROM OPENROWSET(
    BULK 'https://datalakeyourname001.dfs.core.windows.net/clean/customers/year=2025/month=01/*.parquet',
    FORMAT = 'PARQUET'
) AS customers;

Step 3 â€” Compare CSV vs Parquet performance

Run same query against CSV and Parquet.
Note execution time & data scanned.

ğŸ“ Day 2 Deliverables
src/analytics/synapse/day2_parquet_queries.sql
project/week3-synapse/performance/csv_vs_parquet.md


Include:

Execution time

Data scanned (from Synapse UI)

Short conclusion

ğŸŸ¦ DAY 3 â€” Create External Data Source, File Format & Tables

Goal: Build reusable SQL objects instead of ad-hoc OPENROWSET queries.

ğŸ“˜ Study (1â€“1.5 hrs)

Microsoft Learn:

Create external tables using serverless SQL pools
https://learn.microsoft.com/training/modules/create-external-tables-serverless-sql-pools/

ğŸ”§ Hands-On (3 hrs)
Step 1 â€” Create external data source
CREATE EXTERNAL DATA SOURCE eds_datalake
WITH (
    LOCATION = 'https://datalakeyourname001.dfs.core.windows.net'
);

Step 2 â€” Create external file format
CREATE EXTERNAL FILE FORMAT eff_parquet
WITH (
    FORMAT_TYPE = PARQUET
);

Step 3 â€” Create external table
CREATE EXTERNAL TABLE dbo.ext_customers
(
    customer_id INT,
    name VARCHAR(100),
    signup_date DATE
)
WITH (
    LOCATION = '/clean/customers/',
    DATA_SOURCE = eds_datalake,
    FILE_FORMAT = eff_parquet
);

Step 4 â€” Query external table
SELECT COUNT(*) FROM dbo.ext_customers;

ğŸ“ Day 3 Deliverables
src/analytics/synapse/external_objects.sql
src/analytics/synapse/external_tables/ext_customers.sql
project/week3-synapse/day3-external-tables.md


Document:

Why external tables matter

Difference vs OPENROWSET

ğŸŸ¦ DAY 4 â€” Build Analytics Views (Serving Layer)

Goal: Create BI-friendly views and basic star-schema concepts.

ğŸ“˜ Study (1 hr)

Read:

Synapse Serverless best practices

Views vs tables in analytics

ğŸ”§ Hands-On (3â€“4 hrs)
Create curated views
CREATE VIEW dbo.vw_customers
AS
SELECT
    customer_id,
    name,
    signup_date,
    YEAR(signup_date) AS signup_year
FROM dbo.ext_customers;


Query:

SELECT signup_year, COUNT(*) 
FROM dbo.vw_customers
GROUP BY signup_year;

Optional: Join example

If you have transactions:

CREATE VIEW dbo.vw_customer_sales
AS
SELECT
    c.customer_id,
    c.name,
    COUNT(t.transaction_id) AS orders
FROM dbo.ext_customers c
LEFT JOIN dbo.ext_transactions t
    ON c.customer_id = t.customer_id
GROUP BY c.customer_id, c.name;

ğŸ“ Day 4 Deliverables
src/analytics/synapse/views/vw_customers.sql
src/analytics/synapse/views/vw_customer_sales.sql
project/week3-synapse/serving-layer-design.md

ğŸŸ¦ DAY 5 â€” Performance Testing + Documentation + PR

Goal: Treat this like production engineering work.

ğŸ”§ Performance Testing (2 hrs)
Run these comparisons:

OPENROWSET vs External Table

CSV vs Parquet

Partitioned vs non-partitioned

Record:

Execution time

Data scanned

Cost implications

ğŸ“˜ Documentation (2 hrs)

Create:

project/week3-synapse/performance-report.md


Include:

Query screenshots

Observations

Optimization recommendations

ğŸ” GitHub Work

Branch: week3-synapse-serverless

Commit all SQL + docs

Open PR â†’ main

ğŸ“¦ WEEK 3 FINAL DELIVERABLES (BIG PICTURE)

By end of Week 3, your repo contains:

âœ… SQL Code
src/analytics/synapse/
â”œâ”€â”€ day1_openrowset.sql
â”œâ”€â”€ day2_parquet_queries.sql
â”œâ”€â”€ external_objects.sql
â”œâ”€â”€ external_tables/
â”‚   â””â”€â”€ ext_customers.sql
â””â”€â”€ views/
    â”œâ”€â”€ vw_customers.sql
    â””â”€â”€ vw_customer_sales.sql

âœ… Documentation
project/week3-synapse/
â”œâ”€â”€ day1-notes.md
â”œâ”€â”€ day3-external-tables.md
â”œâ”€â”€ serving-layer-design.md
â”œâ”€â”€ performance/
â”‚   â””â”€â”€ performance-report.md
â””â”€â”€ screenshots/

âœ… Skills Gained (DP-700 Relevant)

Serverless SQL analytics

Lakehouse querying patterns

External tables

BI-ready views

Cost-aware performance tuning

Git-based SQL development
